# SynapseUI

**SynapseUI** is a project I've been planning and developing for some time.

The goal is to create a UI to manage AI LLM (Large Language Model) workflows.  
It is the **first and only UI designed to manage models using a chain-based structure**, and it will follow a **node-based** design, inspired by tools like **ComfyUI**.

---

## Example Workflow

```text
[PromptNode]
    |
    | Input: User's natural language message.
    ↓
[IntentClassifierNode]
    |
    | Task: Analyze the intent of the prompt.
    | Model: A small instruct model running on Ollama (e.g., mistral:instruct).
    | Sample output:
    | {
    |   "intent": "generate_image",
    |   "arguments": {
    |     "prompt": "a futuristic robot"
    |   }
    | }
    ↓
+---------------------------------------------+
| intent = "generate_image"  → [ImageGenNode] |
| intent = "caption_image"   → [VisionNode]   |
| intent = "search"          → [ToolNode]     |
| intent = "chat"            → (pass through) |
+---------------------------------------------+
                    ↓
     (Intermediate outputs are gathered and passed on)
                    ↓
         [SystemPromptNode] ← (optional, defines assistant role)
             |
             | Example: "You are a helpful assistant that always answers in markdown."
             ↓
         [MainModelNode]
             |
             | Final response generated by the main LLM (e.g., Mistral, LLaMA2, Gemma)
             ↓
            [RunNode]
             ↓
         [ChatPanel / OutputDisplay]
```

More detail:

```
SYNAPSEUI CHAIN SIMULATION  
(Multimodal, Intent-Driven Routing)

User Prompt:
"What is in this image? Draw it and explain it."

[PromptNode]
Input:

"What is in this image? Draw it and explain it."
→ The prompt is sent to the `IntentClassifierNode`.

---

[IntentClassifierNode]

Task: 

Analyzes the user's prompt and produces a structured JSON output.

Output:

{
  "intent": "vision_and_generate_image",
  "arguments": {
    "image": "base64://....",
    "generate_prompt": "a robot holding a flower",
    "question": "What is in this image?"
  }
}

→ Based on the detected intent, relevant downstream nodes are activated.

---

Activated Nodes:

[VisionNode]  

Input: 
Base64-encoded image

Output:  
"A robot is holding a small white flower in a field."

---

[ImageGenNode]  

Input:
"a robot holding a flower"

Output:

Base64-encoded image (to be passed to the OutputDisplay)

---

[SystemPromptNode]  

Content:  

"You are a helpful assistant who answers in Turkish with clear explanations. If vision input is given, always start by describing the image."

[MainModelNode]

Aggregated Inputs:

- User prompt:  
  "What is in this image? Draw it and explain it."
- Vision caption:  
  "A robot is holding a small white flower in a field."
- System prompt:  
  "You are a helpful assistant who..."

[Model]

`mistral:instruct`

[Generated Response]

The image shows a robot holding a small white flower. This may symbolize the relationship between artificial intelligence and human-like emotions.
---

[RunNode]

- Executes the entire chain  
- Sends the final response to the ChatPanel  
- Sends the generated image to the OutputDisplay

[ChatPanel / OutputDisplay]

- Text response: Displayed to the user  
- Generated image: Displayed to the user  
- Vision caption: Included in model context but hidden from the user
```

![Node example](https://raw.githubusercontent.com/par274/synapseui/main/.github/images/SynapseUINode.png)

he project is in a very early stage for now, I've made the Github page public but there's nothing to test at the moment.
 
I wrote a template engine with a custom syntax structure(XenForo like) for this project, it has a framework that works more container-based, I named it SynapticFramework, but in the end it is a Par3. Currently, it supports structures such as GPU passtrough. There is an Ollama service and I have already written an API management for Ollama on the PHP side.

I uploaded the project with its own Docker manager. The project will work entirely with Docker so that once the project is executable or useful, they can run it in Docker effortlessly. 