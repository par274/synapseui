services:
  svc.synui_llama-swap:
    image: ghcr.io/mostlygeek/llama-swap:cuda
    profiles: [manual]
    volumes:
      - ./.docker/llama-swap/models:/models
      - ./.docker/llama-swap/swap-config.yaml:/app/config.yaml:ro
    container_name: svc.synui_llama-swap
    tty: true
    expose:
      - 8080
    networks:
      - synui_app
      - proxy
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    environment:
      - HF_HOME=/models/huggingface
      - LLAMA_CACHE=/models
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all

networks:
  proxy:
    external: true