services:
  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    profiles: [manual]
    environment:
      LLAMA_ARG_CTX_SIZE: 2048
      LLAMA_ARG_MODEL: /root/.llamacpp/models/gemma-3-1b-it-Q4_K_M.gguf
      LLAMA_ARG_PORT: 11800
      LLAMA_ARG_NO_WEBUI: 1
    volumes:
      - ./.docker/llamacpp/models:/root/.llamacpp/models
    container_name: llamacpp
    tty: true
    ports:
      - 11800:11800
    networks:
      - synui_app
