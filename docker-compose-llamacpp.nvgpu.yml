services:
  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:light-cuda
    environment:
      GGML_CUDA_NO_PINNED: "1"
      LLAMA_ARG_CTX_SIZE: "2048"
      LLAMA_ARG_N_GPU_LAYERS: "99"
    volumes:
      - ./.docker/llamacpp/models:/root/.llamacpp/models
    container_name: llamacpp
    tty: true
    ports:
      - 11800:11800
    networks:
      - synui_app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
